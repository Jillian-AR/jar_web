[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nJillian Andres Rothschild is a Government and Politics PhD student at the University of Maryland."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "website",
    "section": "",
    "text": "---\ntitle: \"Research\"\n---"
  },
  {
    "objectID": "newsletters.html",
    "href": "newsletters.html",
    "title": "Newsletter Networks",
    "section": "",
    "text": "Project Overview\nFor this project, I downloaded Congressional newsletters mentioning “January 6th” from 1/1/21 - 1/31/23. The newsletters are from the DC Inbox dataset (dcinbox.com). I am interested in partisan differences in these newsletters.\nFigure 1 represents the top words in the newsletters by party.\n\n feature chi2 p n_target n_reference 1 democracy 503.13812 0 823 79 2 health 309.64590 0 1243 400 3 insurrection 225.62536 0 324 19 4 assistance 214.39493 0 638 161 5 san 209.42231 0 272 8 6 rescue 200.10376 0 282 15 7 infrastructure 195.72785 0 529 121 8 covid-19 190.15339 0 926 336 9 child 159.86088 0 331 53 10 community 150.06205 0 769 287 11 espaillat 149.64185 0 176 0 12 help 143.30592 0 953 404 13 congresswoman 143.17920 0 417 103 14 bourdeaux 139.43556 0 164 0 15 eligible 138.57628 0 301 52 16 care 135.89958 0 667 243 17 ca 129.40243 0 162 3 18 attack 125.20496 0 567 197 19 funding 117.32170 0 659 258 20 plan 116.01973 0 492 164 21 virginia 114.59655 0 185 17 22 act 114.38955 0 1619 884 23 rhode 111.37088 0 131 0 24 coverage 106.93448 0 173 16 25 manchin 101.41360 0 147 9 26 access 95.91433 0 503 190 27 program 93.82509 0 577 236 28 hampshire 93.51351 0 110 0 29 passport 92.82389 0 176 24 30 manning 86.44960 0 105 1 \n\nI was also interested in seeing the similarity among newsletter authors by party. Figure 2 represents author similarity among Republicans. Representative Liz Cheney defied her party by condemning the January 6th attacks, and perhaps not surprisingly, she is an outlier among Republicans in this dataset. Representative Gaetz and Representative Gosar have reputations for being farther right than most of their co-partisans, and we can see in Figure 2 that they are clustered together.\n[1] 106 \nFigure 3 represents author similarity among Democrats. Senator Joe Manchin often contradicts his party, and we can see he is in his own cluster.\n[1] 126 \n\n\nR Script\n\n## Library to produce network graphs\nlibrary(igraph) \nlibrary(haven)\nlibrary(foreign)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(devtools)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(tm)\nlibrary(readtext)\nlibrary(stringr)\nlibrary(tidytext)\n\n#set your working directory\nsetwd(\"C:/Users/15707/Documents/GVPT Methods\")\n\n#load Data\nnewsletters &lt;- read.csv(\"dcinbox_export_january 6th.csv\")\nrepublicans &lt;- ifelse(newsletters$Party==\"Republican\",1,0) # making a variable for party where republicans are 1 and democrats are 0\nnewsletters$republicans &lt;- republicans\ntext &lt;- newsletters$Body \nmyCorpus &lt;-corpus(text)\ndocvars(myCorpus, \"republicans\") &lt;- republicans\nlast_name &lt;- newsletters$Last.Name\ndocvars(myCorpus, \"last_name\") &lt;- last_name\ntoks_text &lt;- tokens(myCorpus, remove_punct = TRUE) \ndfmat_all &lt;- toks_text %&gt;% \n    dfm() %&gt;% \n    dfm_remove(pattern = c(\"$*\", \"#*\", \"@*\")) %&gt;% \n    dfm_remove(pattern = stopwords(\"en\")) %&gt;% \n    dfm_remove(pattern = stopwords(\"sp\")) # removing symbols and stopwords in english and spanish. Some of the newsletters have sections in spanish.\ndfmat_text &lt;- dfm_trim(dfmat_all, min_termfreq = 10) # trimming data, words used less than 10 times are excluded\n# head(tstat_freq, 32)\ntemp&lt;-dfm_group(dfmat_text, republicans)\n\ndfmat_text %&gt;% \n  textstat_frequency(.,n = 15) %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\") +\n  theme_minimal()\ndocvars(dfmat_text, \"republicans\") &lt;- republicans\n\nhead(textstat_keyness(dfmat_text, target=republicans==0, measure=\"chi2\"), n=30)\ncompareText&lt;-textstat_keyness(dfmat_text, target=republicans==0, measure=\"chi2\")\ntextplot_keyness(compareText,\n                 color = c(\"blue\", \"red\"),\n                 labelcolor = \"gray30\",\n                 labelsize = 4,\n                 n = 30L) # top words by party\n\nrepcor &lt;- corpus_subset(myCorpus, republicans == 1)\ntoksr &lt;- tokens(repcor, remove_punct = TRUE) \ndfmat_r &lt;- toksr %&gt;% \n    dfm() %&gt;% \n    dfm_remove(pattern = c(\"$*\", \"#*\", \"@*\")) %&gt;% \n    dfm_remove(pattern = stopwords(\"en\")) %&gt;% \n    dfm_remove(pattern = stopwords(\"sp\")) \ndfmat_rusers &lt;- dfm_group(dfmat_r, groups = last_name)\nndoc(dfmat_rusers)\ndfmat_rusers &lt;- dfmat_rusers %&gt;% \n    dfm_select(min_nchar = 2) %&gt;% \n    dfm_trim(min_termfreq = 50) # trimming dataset, only words that happen 50 times, helps make sense of data \ndfmat_rusers &lt;- dfmat_rusers[ntoken(dfmat_rusers) &gt; 1000,] # only keeping users with more than 1000 tokens, helps to trim dataset and identify top users\ntstat_dist &lt;- as.dist(textstat_dist(dfmat_rusers))\nuser_clust &lt;- hclust(tstat_dist)\nplot(user_clust)\ndemcor &lt;- corpus_subset(myCorpus, republicans == 0)\ntoksd &lt;- tokens(demcor, remove_punct = TRUE) \ndfmat_dems &lt;- toksd %&gt;% \n    dfm() %&gt;% \n    dfm_remove(pattern = c(\"$*\", \"#*\", \"@*\")) %&gt;% \n    dfm_remove(pattern = stopwords(\"en\")) %&gt;% \n    dfm_remove(pattern = stopwords(\"sp\")) \ndfmat_dusers &lt;- dfm_group(dfmat_dems, groups = last_name)\nndoc(dfmat_dusers)\ndfmat_dusers &lt;- dfmat_dusers %&gt;% \n    dfm_select(min_nchar = 2) %&gt;% \n    dfm_trim(min_termfreq = 50) # trimming dataset, only words that happen 50 times, helps make sense of data  \ndfmat_dusers &lt;- dfmat_dusers[ntoken(dfmat_dusers) &gt; 1000,] # only keeping users with more than 1000 tokens, helps to trim dataset and identify top users\ntstat_dist &lt;- as.dist(textstat_dist(dfmat_dusers))\nuser_clust &lt;- hclust(tstat_dist)\nplot(user_clust)"
  }
]